---
title: About me
layout: home
---
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Z9MML7RXR2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-Z9MML7RXR2');
  </script>
  
  <style>
      .para {
          margin-top: 20px;
      }
  </style>
</head>
<div style="background-color: var(--tf-page-bg-color)" class="bg-gradient py-3">
  <div class="container">

    <h1 class="display-4">Research Statement</h1>
    <div class="para"><b>My future goal:</b> I want to dive deeper into the interdisciplinary fields of linguistics, cognitive science, and computer science to depict a whole picture of human language processing. To achieve this, I want to continue my research in syntactic processing and pragmatic reasoning through  experimental methods and computational modeling. Specifically, I want to examine sentence processing theories with naturalistic data collected from humans. I'm also excited to find insights from LLMs to answer how humans can process languages so effortlessly. Moreover, I'm eager to develop biologically meaningful language models by injecting linguistic constraints into LLMs or buildng probabilistic models according to theories. </div>  

    <div class="para"><b>A summary of my previous research:</b> My previous research focused on large language models (LLMs) and human sentence processing. In my first line of research, I developed machine learning detectors to distinguish the exam essays generated by LLMs from those written by human test takers. I also evaluated the ability of LLMs to understand conversational implicatures with a hand-crafted conversational dataset. In my second line of research, I explored the reason for the inverse correlation between the size, the amount of training data of the LLMs, and their prediction of surprisal. I also modified a computational cognitive model, a left corner parser with distributed associative memory, to evaluate the surprisal account of sentence processing difficulty. The refactored model robustly reveals the activation of memory at the neurological level and the parallel propagation of various analyses with computationally formalized mental states.</div>

    <div class="para"><b>What motivated me to study computational linguistics:</b> My interest in computational linguistics stemmed from my broad interest in linguistic phenomena and theories. Before my undergraduate study, I learned English, Spanish, and Japanese and this experience motivated me to think about the universality across languages, which led me to the study of linguistics. In my freshman year, I was intrigued by syntax that transforms the richness of language into a computable representation consisting of discrete and combinatory units. My interest grew along with my access to syntactic phenomena such as filler-gap constructions, constituent movements, and garden-path sentences around the question of how they are processed by humans. I was also interested in semantic and pragmatic disambiguation, especially how people understand implicatures in daily communication. My access to generative grammar and the cooperative principle allowed me to examine these phenomena through a theoretical lens. However, I found myself unable to be totally convinced by theories without data-driven evidence. The truthfulness of theories, in my view, hinges on their statistical significance derived from real-world data. To validate the existence of frameworks or cognitive processes posited by linguistic theories, I embarked on a journey into computational linguistics. </div>
    
    

    
    
  </div>
</div>